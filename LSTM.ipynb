{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:48:01.754088Z",
     "iopub.status.busy": "2024-12-14T04:48:01.753806Z",
     "iopub.status.idle": "2024-12-14T04:48:01.758463Z",
     "shell.execute_reply": "2024-12-14T04:48:01.757636Z",
     "shell.execute_reply.started": "2024-12-14T04:48:01.754064Z"
    },
    "id": "5A8FsobQshtj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:48:03.453647Z",
     "iopub.status.busy": "2024-12-14T04:48:03.453200Z",
     "iopub.status.idle": "2024-12-14T04:48:03.457652Z",
     "shell.execute_reply": "2024-12-14T04:48:03.456766Z",
     "shell.execute_reply.started": "2024-12-14T04:48:03.453613Z"
    },
    "id": "r44Ayu9eXwIS"
   },
   "outputs": [],
   "source": [
    "MODELNAME1 = 'iwslt15-en-vi-rnn.model'\n",
    "MODELNAME2 = 'iwslt15-en-vi-lstm.model'\n",
    "EPOCH = 10\n",
    "BATCHSIZE = 32\n",
    "LR = 0.0001\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHkyfd1b9akh"
   },
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckoFWbwm9-w-"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:48:06.238820Z",
     "iopub.status.busy": "2024-12-14T04:48:06.238494Z",
     "iopub.status.idle": "2024-12-14T04:48:08.118069Z",
     "shell.execute_reply": "2024-12-14T04:48:08.117404Z",
     "shell.execute_reply.started": "2024-12-14T04:48:06.238792Z"
    }
   },
   "outputs": [],
   "source": [
    "def readfile(name_file):\n",
    "    with open(name_file) as f:\n",
    "        LINES = [line.split() for line in f.readlines()]\n",
    "    return LINES\n",
    "train_en = readfile(\"/kaggle/input/data-en-vi/dataset/train.en.txt\")\n",
    "train_vi = readfile(\"/kaggle/input/data-en-vi/dataset/train.vi.txt\")\n",
    "test_en = readfile(\"/kaggle/input/data-en-vi/dataset/tst2013.en.txt\")\n",
    "test_vi = readfile(\"/kaggle/input/data-en-vi/dataset/tst2013.vi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:48:09.951442Z",
     "iopub.status.busy": "2024-12-14T04:48:09.951132Z",
     "iopub.status.idle": "2024-12-14T04:48:09.959599Z",
     "shell.execute_reply": "2024-12-14T04:48:09.958634Z",
     "shell.execute_reply.started": "2024-12-14T04:48:09.951417Z"
    },
    "id": "g3-yzWYtv5-E",
    "outputId": "ebad4a64-b92b-4990-cca6-b60e1667f1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline']\n",
      "['Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu']\n",
      "['In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.']\n",
      "['Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.']\n",
      "['I', '&apos;d', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.']\n",
      "['Tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to', 'lớn', 'của', 'những', 'nỗ', 'lực', 'khoa', 'học', 'đã', 'góp', 'phần', 'làm', 'nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.']\n",
      "['Headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.']\n",
      "['Có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất', 'lượng', 'không', 'khí', 'hay', 'khói', 'bụi', '.']\n",
      "['They', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.']\n",
      "['Cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh', 'vực', 'trong', 'ngành', 'khoa', 'học', 'khí', 'quyển', '.']\n",
      "['Recently', 'the', 'headlines', 'looked', 'like', 'this', 'when', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', ',', 'or', 'IPCC', ',', 'put', 'out', 'their', 'report', 'on', 'the', 'state', 'of', 'understanding', 'of', 'the', 'atmospheric', 'system', '.']\n",
      "['Các', 'tiêu', 'đề', 'gần', 'đây', 'trông', 'như', 'thế', 'này', 'khi', 'Ban', 'Điều', 'hành', 'Biến', 'đổi', 'khí', 'hậu', 'Liên', 'chính', 'phủ', ',', 'gọi', 'tắt', 'là', 'IPCC', 'đưa', 'ra', 'bài', 'nghiên', 'cứu', 'của', 'họ', 'về', 'hệ', 'thống', 'khí', 'quyển', '.']\n",
      "['That', 'report', 'was', 'written', 'by', '620', 'scientists', 'from', '40', 'countries', '.']\n",
      "['Nghiên', 'cứu', 'được', 'viết', 'bởi', '620', 'nhà', 'khoa', 'học', 'từ', '40', 'quốc', 'gia', 'khác', 'nhau', '.']\n",
      "['They', 'wrote', 'almost', 'a', 'thousand', 'pages', 'on', 'the', 'topic', '.']\n",
      "['Họ', 'viết', 'gần', '1000', 'trang', 'về', 'chủ', 'đề', 'này', '.']\n",
      "['And', 'all', 'of', 'those', 'pages', 'were', 'reviewed', 'by', 'another', '400-plus', 'scientists', 'and', 'reviewers', ',', 'from', '113', 'countries', '.']\n",
      "['Và', 'tất', 'cả', 'các', 'trang', 'đều', 'được', 'xem', 'xét', 'bởi', '400', 'khoa', 'học', 'gia', 'và', 'nhà', 'phê', 'bình', 'khác', 'từ', '113', 'quốc', 'gia', '.']\n",
      "['It', '&apos;s', 'a', 'big', 'community', '.', 'It', '&apos;s', 'such', 'a', 'big', 'community', ',', 'in', 'fact', ',', 'that', 'our', 'annual', 'gathering', 'is', 'the', 'largest', 'scientific', 'meeting', 'in', 'the', 'world', '.']\n",
      "['Đó', 'là', 'cả', 'một', 'cộng', 'đồng', 'lớn', ',', 'lớn', 'đến', 'nỗi', 'trên', 'thực', 'tế', 'cuộc', 'tụ', 'hội', 'hằng', 'năm', 'của', 'chúng', 'tôi', 'là', 'hội', 'nghị', 'khoa', 'học', '&#91;', 'tự', 'nhiên', '&#93;', 'lớn', 'nhất', 'thế', 'giới', '.']\n",
      "# of line 133317 133317 1268 1268\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(train_en[i])\n",
    "  print(train_vi[i])\n",
    "print('# of line', len(train_en), len(train_vi), len(test_en), len(test_vi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mP7hrjhA-B0e"
   },
   "source": [
    "## Make Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:49:23.072911Z",
     "iopub.status.busy": "2024-12-14T04:49:23.072596Z",
     "iopub.status.idle": "2024-12-14T04:49:24.245997Z",
     "shell.execute_reply": "2024-12-14T04:49:24.245056Z",
     "shell.execute_reply.started": "2024-12-14T04:49:23.072886Z"
    },
    "id": "13w63o7lxDAy",
    "outputId": "8362f4ed-d200-4278-a392-c8240c44c023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size en:  24420\n",
      "vocab size vi:  10666\n"
     ]
    }
   ],
   "source": [
    "def make_vocab(train_data, min_freq):\n",
    "  vocab = {}\n",
    "  for tokenlist in train_data:\n",
    "    for token in tokenlist:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = 0\n",
    "      vocab[token] += 1 # Đếm các từ data \n",
    "  vocablist = [('<unk>',0),('<pad>', 0), ('<cls>', 0), ('<eos>', 0)] # Danh sách các tuple đặc trưng trước\n",
    "  vocabidx = {} # dict của tuple\n",
    "  for token, freq in vocab.items(): # Từ đó , với số lượng của từ đó trong train data \n",
    "    if freq >= min_freq: # Số lượng từ mà > 3\n",
    "      idx = len(vocablist) # đang là 4 \n",
    "      vocablist.append((token, freq)) #Từ đó , với số lượng của từ đó trong train data \n",
    "      vocabidx[token] = idx  # Từ điển (dict)  \n",
    "  vocabidx['<unk>'] = 0\n",
    "  vocabidx['<pad>'] = 1\n",
    "  vocabidx['<cls>'] = 2\n",
    "  vocabidx['<eos>'] = 3\n",
    "  return vocablist, vocabidx\n",
    "\n",
    "vocablist_en, vocabidx_en = make_vocab(train_en, 3)\n",
    "vocablist_vi, vocabidx_vi = make_vocab(train_vi, 3)\n",
    "\n",
    "print('vocab size en: ', len(vocablist_en))\n",
    "print('vocab size vi: ', len(vocablist_vi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:48:21.989808Z",
     "iopub.status.busy": "2024-12-14T04:48:21.989436Z",
     "iopub.status.idle": "2024-12-14T04:48:23.318362Z",
     "shell.execute_reply": "2024-12-14T04:48:23.317471Z",
     "shell.execute_reply.started": "2024-12-14T04:48:21.989777Z"
    },
    "id": "O-GCG1mfzBZL",
    "outputId": "fe9f2593-859e-4f12-e3bc-0d2dcfc83ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', 'Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n",
      "['<cls>', 'Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu', '<eos>']\n",
      "['<cls>', 'When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', '<unk>', '.', '&quot;', '<eos>']\n",
      "['<cls>', 'In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.', '<eos>']\n",
      "['<cls>', 'Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.', '<eos>']\n",
      "['<cls>', 'And', 'I', 'was', 'very', 'proud', '.', '<eos>']\n",
      "['<cls>', 'I', '&apos;d', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.', '<eos>']\n",
      "['<cls>', 'Tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to', 'lớn', 'của', 'những', 'nỗ', 'lực', 'khoa', 'học', 'đã', 'góp', 'phần', 'làm', 'nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.', '<eos>']\n",
      "['<cls>', 'In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', '<unk>', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.', '<eos>']\n",
      "['<cls>', '<unk>', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.', '<eos>']\n",
      "['<cls>', 'Có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất', 'lượng', 'không', 'khí', 'hay', 'khói', 'bụi', '.', '<eos>']\n",
      "['<cls>', 'Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>']\n",
      "['<cls>', 'They', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.', '<eos>']\n",
      "['<cls>', 'Cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh', 'vực', 'trong', 'ngành', 'khoa', 'học', 'khí', 'quyển', '.', '<eos>']\n",
      "['<cls>', 'When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data, vocabidx):\n",
    "  rr = []\n",
    "  for tokenlist in data:\n",
    "    tkl = ['<cls>']\n",
    "    for token in tokenlist:\n",
    "      tkl.append(token if token in vocabidx else '<unk>')\n",
    "    tkl.append('<eos>')\n",
    "    rr.append((tkl))\n",
    "  return rr\n",
    " \n",
    "train_en_prep = preprocess(train_en, vocabidx_en)\n",
    "train_vi_prep = preprocess(train_vi, vocabidx_vi)\n",
    "test_en_prep = preprocess(test_en, vocabidx_en)\n",
    "for i in range(5):\n",
    "  print(train_en_prep[i])\n",
    "  print(train_vi_prep[i])\n",
    "  print(test_en_prep[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuKn_t_x-EdH"
   },
   "source": [
    "## Make batch & Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:48:25.766395Z",
     "iopub.status.busy": "2024-12-14T04:48:25.766070Z",
     "iopub.status.idle": "2024-12-14T04:48:26.303940Z",
     "shell.execute_reply": "2024-12-14T04:48:26.303131Z",
     "shell.execute_reply.started": "2024-12-14T04:48:25.766364Z"
    },
    "id": "6ZtKxScuzzJA",
    "outputId": "08f4a59c-a6ac-479b-b935-fa69c7c3d565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
      "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
      "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
      "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
      "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
      "(['<cls>', 'When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', '<unk>', '.', '&quot;', '<eos>'], ['When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', 'Envy', '.', '&quot;'], ['Khi', 'tôi', 'còn', 'nhỏ', ',', 'Tôi', 'nghĩ', 'rằng', 'BắcTriều', 'Tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot;', 'Chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot;'])\n",
      "(['<cls>', 'And', 'I', 'was', 'very', 'proud', '.', '<eos>'], ['And', 'I', 'was', 'very', 'proud', '.'], ['Tôi', 'đã', 'rất', 'tự', 'hào', 'về', 'đất', 'nước', 'tôi', '.'])\n",
      "(['<cls>', 'In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', '<unk>', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.', '<eos>'], ['In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', 'Il-Sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.'], ['Ở', 'trường', ',', 'chúng', 'tôi', 'dành', 'rất', 'nhiều', 'thời', 'gian', 'để', 'học', 'về', 'cuộc', 'đời', 'của', 'chủ', 'tịch', 'Kim', 'II-', 'Sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế', 'giới', 'bên', 'ngoài', ',', 'ngoại', 'trừ', 'việc', 'Hoa', 'Kỳ', ',', 'Hàn', 'Quốc', 'và', 'Nhật', 'Bản', 'là', 'kẻ', 'thù', 'của', 'chúng', 'tôi', '.'])\n",
      "(['<cls>', 'Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>'], ['Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.'], ['Mặc', 'dù', 'tôi', 'đã', 'từng', 'tự', 'hỏi', 'không', 'biết', 'thế', 'giới', 'bên', 'ngoài', 'kia', 'như', 'thế', 'nào', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'rằng', 'mình', 'sẽ', 'sống', 'cả', 'cuộc', 'đời', 'ở', 'BắcTriều', 'Tiên', ',', 'cho', 'tới', 'khi', 'tất', 'cả', 'mọi', 'thứ', 'đột', 'nhiên', 'thay', 'đổi', '.'])\n",
      "(['<cls>', 'When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.', '<eos>'], ['When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.'], ['Khi', 'tôi', 'lên', '7', ',', 'tôi', 'chứng', 'kiến', 'cảnh', 'người', 'ta', 'xử', 'bắn', 'công', 'khai', 'lần', 'đầu', 'tiên', 'trong', 'đời', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'cuộc', 'sống', 'của', 'mình', 'ở', 'đây', 'là', 'hoàn', 'toàn', 'bình', 'thường', '.'])\n"
     ]
    }
   ],
   "source": [
    "train_data = list(zip(train_en_prep, train_vi_prep))\n",
    "train_data.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
    "\n",
    "test_data = list(zip(test_en_prep, test_en, test_vi))\n",
    "\n",
    "for i in range(5):\n",
    "  print(train_data[i])\n",
    "for i in range(5):\n",
    "  print(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:48:28.930110Z",
     "iopub.status.busy": "2024-12-14T04:48:28.929821Z",
     "iopub.status.idle": "2024-12-14T04:48:29.030917Z",
     "shell.execute_reply": "2024-12-14T04:48:29.030029Z",
     "shell.execute_reply.started": "2024-12-14T04:48:28.930086Z"
    },
    "id": "cihKDz3l1aGC",
    "outputId": "299f3834-c203-4835-b581-c8376e39e09c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
      "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n"
     ]
    }
   ],
   "source": [
    "def make_batch(data, batchsize):\n",
    "  bb = []\n",
    "  ben = []\n",
    "  bvi = []\n",
    "  for en,vi in data:\n",
    "    ben.append(en)\n",
    "    bvi.append(vi)\n",
    "    if len(ben) >= batchsize:\n",
    "      bb.append((ben, bvi))\n",
    "      ben = []\n",
    "      bvi = []\n",
    "  if len(ben) > 0:\n",
    "    bb.append((ben, bvi))\n",
    "  return bb\n",
    "\n",
    "train_data = make_batch(train_data, BATCHSIZE)\n",
    "\n",
    "for i in range(2):\n",
    "  print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:49:42.255401Z",
     "iopub.status.busy": "2024-12-14T04:49:42.255094Z",
     "iopub.status.idle": "2024-12-14T04:49:42.386619Z",
     "shell.execute_reply": "2024-12-14T04:49:42.385646Z",
     "shell.execute_reply.started": "2024-12-14T04:49:42.255375Z"
    },
    "id": "PT3mMydz2Jee",
    "outputId": "e6c86f31-a39d-413b-dd0a-e92a0ec2b3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
      "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
      "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n"
     ]
    }
   ],
   "source": [
    "def padding_batch(b):\n",
    "  maxlen = max([len(x) for x in b])\n",
    "  for tkl in b:\n",
    "    for i in range(maxlen - len(tkl)):\n",
    "      tkl.append('<pad>')\n",
    "\n",
    "def padding(bb):\n",
    "  for ben, bvi in bb:\n",
    "    padding_batch(ben)\n",
    "    padding_batch(bvi)\n",
    "\n",
    "padding(train_data)\n",
    "\n",
    "for i in range(3):\n",
    "  print(train_data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:49:46.263968Z",
     "iopub.status.busy": "2024-12-14T04:49:46.263650Z",
     "iopub.status.idle": "2024-12-14T04:49:48.730898Z",
     "shell.execute_reply": "2024-12-14T04:49:48.729919Z",
     "shell.execute_reply.started": "2024-12-14T04:49:46.263942Z"
    },
    "id": "cXta_9yt21KC",
    "outputId": "cbf06c33-c163-4c82-8867-b05b27e78354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
      "([2, 444, 49, 96, 678, 16, 49, 884, 429, 823, 96, 22, 1203, 28, 22, 203, 16, 70, 49, 722, 218, 2403, 10, 2271, 178, 545, 9225, 868, 0, 48, 545, 3], ['When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', 'Envy', '.', '&quot;'], ['Khi', 'tôi', 'còn', 'nhỏ', ',', 'Tôi', 'nghĩ', 'rằng', 'BắcTriều', 'Tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot;', 'Chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot;'])\n",
      "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
      "([2, 109, 49, 96, 198, 6154, 48, 3], ['And', 'I', 'was', 'very', 'proud', '.'], ['Tôi', 'đã', 'rất', 'tự', 'hào', 'về', 'đất', 'nước', 'tôi', '.'])\n",
      "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
      "([2, 13, 702, 16, 171, 1625, 10, 1318, 21, 365, 1830, 22, 625, 21, 8919, 0, 16, 442, 171, 186, 1834, 210, 56, 22, 1737, 128, 16, 1741, 58, 1317, 16, 1258, 1259, 16, 1512, 76, 22, 2722, 48, 3], ['In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', 'Il-Sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.'], ['Ở', 'trường', ',', 'chúng', 'tôi', 'dành', 'rất', 'nhiều', 'thời', 'gian', 'để', 'học', 'về', 'cuộc', 'đời', 'của', 'chủ', 'tịch', 'Kim', 'II-', 'Sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế', 'giới', 'bên', 'ngoài', ',', 'ngoại', 'trừ', 'việc', 'Hoa', 'Kỳ', ',', 'Hàn', 'Quốc', 'và', 'Nhật', 'Bản', 'là', 'kẻ', 'thù', 'của', 'chúng', 'tôi', '.'])\n"
     ]
    }
   ],
   "source": [
    "train_data = [([[vocabidx_en[token] for token in tokenlist] for tokenlist in ben],\n",
    "               [[vocabidx_vi[token] for token in tokenlist] for tokenlist in bvi]) for ben, bvi in train_data]\n",
    "test_data = [([vocabidx_en[token] for token in enprep], en, vi) for enprep, en, vi in test_data]\n",
    "for i in range(3):\n",
    "  print(train_data[i])\n",
    "  print(test_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhUjxwDaXIiM"
   },
   "source": [
    "#Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hDdnYk1XmUM"
   },
   "source": [
    "#Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:49:52.016471Z",
     "iopub.status.busy": "2024-12-14T04:49:52.016172Z",
     "iopub.status.idle": "2024-12-14T04:49:52.253276Z",
     "shell.execute_reply": "2024-12-14T04:49:52.252364Z",
     "shell.execute_reply.started": "2024-12-14T04:49:52.016444Z"
    },
    "id": "WDi-Nv7EXJss",
    "outputId": "5f74323f-221b-4156-b178-73082f0ffa62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLSTM(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (embedding): Embedding(24420, 300)\n",
      "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "device=DEVICE\n",
    "class EncoderLSTM(nn.Module):\n",
    "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "    super(EncoderLSTM, self).__init__()\n",
    "\n",
    "    # Size of the one hot vectors that will be the input to the encoder\n",
    "    #self.input_size = input_size\n",
    "\n",
    "    # Output size of the word embedding NN\n",
    "    #self.embedding_size = embedding_size\n",
    "\n",
    "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    # Number of layers in the lstm\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # Regularization parameter\n",
    "    self.dropout = nn.Dropout(p)\n",
    "    self.tag = True\n",
    "\n",
    "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    \n",
    "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
    "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
    "\n",
    "  # Shape of x (26, 32) [Sequence_length, batch_size]\n",
    "  def forward(self, x):\n",
    "\n",
    "    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "    \n",
    "    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n",
    "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n",
    "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "    return hidden_state, cell_state\n",
    "\n",
    "input_size_encoder = len(vocablist_en)\n",
    "encoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "\n",
    "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
    "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
    "print(encoder_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7tPFHC3Xj-E"
   },
   "source": [
    "#Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:49:55.268934Z",
     "iopub.status.busy": "2024-12-14T04:49:55.268530Z",
     "iopub.status.idle": "2024-12-14T04:49:55.555288Z",
     "shell.execute_reply": "2024-12-14T04:49:55.554444Z",
     "shell.execute_reply.started": "2024-12-14T04:49:55.268884Z"
    },
    "id": "BYbJhVBiXoyl",
    "outputId": "b6bea5a2-3f73-498d-bc5d-cc8fb4808948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderLSTM(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (embedding): Embedding(10666, 300)\n",
      "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
      "  (fc): Linear(in_features=1024, out_features=10666, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
    "    super(DecoderLSTM, self).__init__()\n",
    "\n",
    "    # Size of the one hot vectors that will be the input to the encoder\n",
    "    #self.input_size = input_size\n",
    "\n",
    "    # Output size of the word embedding NN\n",
    "    #self.embedding_size = embedding_size\n",
    "\n",
    "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    # Number of layers in the lstm\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
    "    self.output_size = output_size\n",
    "\n",
    "    # Regularization parameter\n",
    "    self.dropout = nn.Dropout(p)\n",
    "\n",
    "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
    "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
    "\n",
    "    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  # Shape of x (32) [batch_size]\n",
    "  def forward(self, x, hidden_state, cell_state):\n",
    "\n",
    "    # Shape of x (1, 32) [1, batch_size]\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
    "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
    "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
    "\n",
    "    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
    "    predictions = self.fc(outputs)\n",
    "\n",
    "    # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
    "    predictions = predictions.squeeze(0)\n",
    "\n",
    "    return predictions, hidden_state, cell_state\n",
    "\n",
    "input_size_decoder = len(vocablist_vi)\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "decoder_dropout = 0.5\n",
    "output_size = len(vocablist_vi)\n",
    "\n",
    "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
    "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
    "print(decoder_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-b4PMxVYIvt"
   },
   "source": [
    "#TOnghop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:50:02.914230Z",
     "iopub.status.busy": "2024-12-14T04:50:02.913939Z",
     "iopub.status.idle": "2024-12-14T04:50:02.920457Z",
     "shell.execute_reply": "2024-12-14T04:50:02.919619Z",
     "shell.execute_reply.started": "2024-12-14T04:50:02.914205Z"
    },
    "id": "FfkntoJaYKY0"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
    "    super(Seq2Seq, self).__init__()\n",
    "    self.Encoder_LSTM = Encoder_LSTM\n",
    "    self.Decoder_LSTM = Decoder_LSTM\n",
    "\n",
    "  def forward(self, source, target, tfr=0.5):\n",
    "    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n",
    "    batch_size = source.shape[1]\n",
    "\n",
    "    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
    "    target_len = target.shape[0]\n",
    "    target_vocab_size = len(vocablist_vi)\n",
    "    \n",
    "    # Shape --> outputs (14, 32, 5766) \n",
    "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
    "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
    "\n",
    "    # Shape of x (32 elements)\n",
    "    x = target[0] # Trigger token <SOS>\n",
    "\n",
    "    for i in range(1, target_len):\n",
    "      # Shape --> output (32, 5766) \n",
    "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
    "      outputs[i] = output\n",
    "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "    # Shape --> outputs (14, 32, 5766) \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQXwpJpwPfyD"
   },
   "source": [
    "#TRain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:50:05.218770Z",
     "iopub.status.busy": "2024-12-14T04:50:05.218429Z",
     "iopub.status.idle": "2024-12-14T04:50:05.226443Z",
     "shell.execute_reply": "2024-12-14T04:50:05.225685Z",
     "shell.execute_reply.started": "2024-12-14T04:50:05.218743Z"
    },
    "id": "SVL4PVCWYTi8"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "step = 0\n",
    "device=DEVICE\n",
    "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "pad_idx = vocabidx_vi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:50:07.964909Z",
     "iopub.status.busy": "2024-12-14T04:50:07.964473Z",
     "iopub.status.idle": "2024-12-14T04:50:07.970350Z",
     "shell.execute_reply": "2024-12-14T04:50:07.969518Z",
     "shell.execute_reply.started": "2024-12-14T04:50:07.964875Z"
    },
    "id": "QiLWD1xvYYON",
    "outputId": "2cd40148-c087-4ae2-b137-165c9ada6b58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (Encoder_LSTM): EncoderLSTM(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(24420, 300)\n",
       "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (Decoder_LSTM): DecoderLSTM(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(10666, 300)\n",
       "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=1024, out_features=10666, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:50:13.898340Z",
     "iopub.status.busy": "2024-12-14T04:50:13.897983Z",
     "iopub.status.idle": "2024-12-14T04:50:13.905599Z",
     "shell.execute_reply": "2024-12-14T04:50:13.904672Z",
     "shell.execute_reply.started": "2024-12-14T04:50:13.898306Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    N_EPOCHS = 5 \n",
    "    \n",
    "    #----------------------------------------------------TRAIN-------------------------------------------\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        batchloss = 0\n",
    "        total_batches = len(train_data)  # Total number of batches in the training data\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} started...\")  # Inform when a new epoch starts\n",
    "        \n",
    "        # Loop through each batch in the training data\n",
    "        for batch_idx, (en, vi) in enumerate(train_data):\n",
    "            # Convert sentences to tensors and move them to the appropriate device\n",
    "            en = torch.tensor(en, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
    "            vi = torch.tensor(vi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            y = model(en, vi)\n",
    "            \n",
    "            # Calculate output and target for loss computation\n",
    "            output = y[1:].reshape(-1, y.shape[2])\n",
    "            target = vi[1:].reshape(-1)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            batchloss += loss.item()\n",
    "\n",
    "            # Print the batch progress\n",
    "            if batch_idx % 100 == 0:  # Print every 100 batches\n",
    "                print(f\"Epoch {epoch + 1}/{N_EPOCHS}, Batch {batch_idx}/{total_batches}, Loss: {batchloss/(batch_idx+1):.4f}\")\n",
    "        \n",
    "        # Print total loss for the epoch\n",
    "        print(f\"Epoch {epoch + 1} completed, Total Loss: {batchloss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:50:17.657872Z",
     "iopub.status.busy": "2024-12-14T04:50:17.657527Z",
     "iopub.status.idle": "2024-12-14T05:34:05.662443Z",
     "shell.execute_reply": "2024-12-14T05:34:05.661439Z",
     "shell.execute_reply.started": "2024-12-14T04:50:17.657843Z"
    },
    "id": "cjjMlAGRaQ0T",
    "outputId": "ee8f99c5-73d7-4f1d-f46e-792220d69c8e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 started...\n",
      "Epoch 1/5, Batch 0/4167, Loss: 9.2939\n",
      "Epoch 1/5, Batch 100/4167, Loss: 5.2710\n",
      "Epoch 1/5, Batch 200/4167, Loss: 5.2836\n",
      "Epoch 1/5, Batch 300/4167, Loss: 5.2428\n",
      "Epoch 1/5, Batch 400/4167, Loss: 5.2941\n",
      "Epoch 1/5, Batch 500/4167, Loss: 5.3463\n",
      "Epoch 1/5, Batch 600/4167, Loss: 5.3488\n",
      "Epoch 1/5, Batch 700/4167, Loss: 5.3968\n",
      "Epoch 1/5, Batch 800/4167, Loss: 5.4120\n",
      "Epoch 1/5, Batch 900/4167, Loss: 5.4503\n",
      "Epoch 1/5, Batch 1000/4167, Loss: 5.4571\n",
      "Epoch 1/5, Batch 1100/4167, Loss: 5.4781\n",
      "Epoch 1/5, Batch 1200/4167, Loss: 5.4783\n",
      "Epoch 1/5, Batch 1300/4167, Loss: 5.4962\n",
      "Epoch 1/5, Batch 1400/4167, Loss: 5.4950\n",
      "Epoch 1/5, Batch 1500/4167, Loss: 5.5082\n",
      "Epoch 1/5, Batch 1600/4167, Loss: 5.5088\n",
      "Epoch 1/5, Batch 1700/4167, Loss: 5.5166\n",
      "Epoch 1/5, Batch 1800/4167, Loss: 5.5228\n",
      "Epoch 1/5, Batch 1900/4167, Loss: 5.5259\n",
      "Epoch 1/5, Batch 2000/4167, Loss: 5.5329\n",
      "Epoch 1/5, Batch 2100/4167, Loss: 5.5366\n",
      "Epoch 1/5, Batch 2200/4167, Loss: 5.5397\n",
      "Epoch 1/5, Batch 2300/4167, Loss: 5.5441\n",
      "Epoch 1/5, Batch 2400/4167, Loss: 5.5485\n",
      "Epoch 1/5, Batch 2500/4167, Loss: 5.5515\n",
      "Epoch 1/5, Batch 2600/4167, Loss: 5.5535\n",
      "Epoch 1/5, Batch 2700/4167, Loss: 5.5550\n",
      "Epoch 1/5, Batch 2800/4167, Loss: 5.5567\n",
      "Epoch 1/5, Batch 2900/4167, Loss: 5.5579\n",
      "Epoch 1/5, Batch 3000/4167, Loss: 5.5607\n",
      "Epoch 1/5, Batch 3100/4167, Loss: 5.5626\n",
      "Epoch 1/5, Batch 3200/4167, Loss: 5.5644\n",
      "Epoch 1/5, Batch 3300/4167, Loss: 5.5649\n",
      "Epoch 1/5, Batch 3400/4167, Loss: 5.5659\n",
      "Epoch 1/5, Batch 3500/4167, Loss: 5.5665\n",
      "Epoch 1/5, Batch 3600/4167, Loss: 5.5679\n",
      "Epoch 1/5, Batch 3700/4167, Loss: 5.5684\n",
      "Epoch 1/5, Batch 3800/4167, Loss: 5.5684\n",
      "Epoch 1/5, Batch 3900/4167, Loss: 5.5686\n",
      "Epoch 1/5, Batch 4000/4167, Loss: 5.5691\n",
      "Epoch 1/5, Batch 4100/4167, Loss: 5.5703\n",
      "Epoch 1 completed, Total Loss: 23220.4362\n",
      "Epoch 2/5 started...\n",
      "Epoch 2/5, Batch 0/4167, Loss: 5.8926\n",
      "Epoch 2/5, Batch 100/4167, Loss: 4.0303\n",
      "Epoch 2/5, Batch 200/4167, Loss: 4.1731\n",
      "Epoch 2/5, Batch 300/4167, Loss: 4.2069\n",
      "Epoch 2/5, Batch 400/4167, Loss: 4.3224\n",
      "Epoch 2/5, Batch 500/4167, Loss: 4.4022\n",
      "Epoch 2/5, Batch 600/4167, Loss: 4.4247\n",
      "Epoch 2/5, Batch 700/4167, Loss: 4.4859\n",
      "Epoch 2/5, Batch 800/4167, Loss: 4.5035\n",
      "Epoch 2/5, Batch 900/4167, Loss: 4.5521\n",
      "Epoch 2/5, Batch 1000/4167, Loss: 4.5657\n",
      "Epoch 2/5, Batch 1100/4167, Loss: 4.6030\n",
      "Epoch 2/5, Batch 1200/4167, Loss: 4.6183\n",
      "Epoch 2/5, Batch 1300/4167, Loss: 4.6513\n",
      "Epoch 2/5, Batch 1400/4167, Loss: 4.6613\n",
      "Epoch 2/5, Batch 1500/4167, Loss: 4.6863\n",
      "Epoch 2/5, Batch 1600/4167, Loss: 4.6967\n",
      "Epoch 2/5, Batch 1700/4167, Loss: 4.7174\n",
      "Epoch 2/5, Batch 1800/4167, Loss: 4.7346\n",
      "Epoch 2/5, Batch 1900/4167, Loss: 4.7494\n",
      "Epoch 2/5, Batch 2000/4167, Loss: 4.7665\n",
      "Epoch 2/5, Batch 2100/4167, Loss: 4.7791\n",
      "Epoch 2/5, Batch 2200/4167, Loss: 4.7911\n",
      "Epoch 2/5, Batch 2300/4167, Loss: 4.8055\n",
      "Epoch 2/5, Batch 2400/4167, Loss: 4.8195\n",
      "Epoch 2/5, Batch 2500/4167, Loss: 4.8313\n",
      "Epoch 2/5, Batch 2600/4167, Loss: 4.8433\n",
      "Epoch 2/5, Batch 2700/4167, Loss: 4.8553\n",
      "Epoch 2/5, Batch 2800/4167, Loss: 4.8668\n",
      "Epoch 2/5, Batch 2900/4167, Loss: 4.8782\n",
      "Epoch 2/5, Batch 3000/4167, Loss: 4.8889\n",
      "Epoch 2/5, Batch 3100/4167, Loss: 4.8999\n",
      "Epoch 2/5, Batch 3200/4167, Loss: 4.9108\n",
      "Epoch 2/5, Batch 3300/4167, Loss: 4.9205\n",
      "Epoch 2/5, Batch 3400/4167, Loss: 4.9313\n",
      "Epoch 2/5, Batch 3500/4167, Loss: 4.9411\n",
      "Epoch 2/5, Batch 3600/4167, Loss: 4.9511\n",
      "Epoch 2/5, Batch 3700/4167, Loss: 4.9601\n",
      "Epoch 2/5, Batch 3800/4167, Loss: 4.9691\n",
      "Epoch 2/5, Batch 3900/4167, Loss: 4.9785\n",
      "Epoch 2/5, Batch 4000/4167, Loss: 4.9877\n",
      "Epoch 2/5, Batch 4100/4167, Loss: 4.9973\n",
      "Epoch 2 completed, Total Loss: 20856.2472\n",
      "Epoch 3/5 started...\n",
      "Epoch 3/5, Batch 0/4167, Loss: 2.3264\n",
      "Epoch 3/5, Batch 100/4167, Loss: 3.3048\n",
      "Epoch 3/5, Batch 200/4167, Loss: 3.6420\n",
      "Epoch 3/5, Batch 300/4167, Loss: 3.7343\n",
      "Epoch 3/5, Batch 400/4167, Loss: 3.8762\n",
      "Epoch 3/5, Batch 500/4167, Loss: 3.9855\n",
      "Epoch 3/5, Batch 600/4167, Loss: 4.0220\n",
      "Epoch 3/5, Batch 700/4167, Loss: 4.0934\n",
      "Epoch 3/5, Batch 800/4167, Loss: 4.1210\n",
      "Epoch 3/5, Batch 900/4167, Loss: 4.1816\n",
      "Epoch 3/5, Batch 1000/4167, Loss: 4.1989\n",
      "Epoch 3/5, Batch 1100/4167, Loss: 4.2411\n",
      "Epoch 3/5, Batch 1200/4167, Loss: 4.2596\n",
      "Epoch 3/5, Batch 1300/4167, Loss: 4.3006\n",
      "Epoch 3/5, Batch 1400/4167, Loss: 4.3158\n",
      "Epoch 3/5, Batch 1500/4167, Loss: 4.3455\n",
      "Epoch 3/5, Batch 1600/4167, Loss: 4.3645\n",
      "Epoch 3/5, Batch 1700/4167, Loss: 4.3891\n",
      "Epoch 3/5, Batch 1800/4167, Loss: 4.4107\n",
      "Epoch 3/5, Batch 1900/4167, Loss: 4.4302\n",
      "Epoch 3/5, Batch 2000/4167, Loss: 4.4511\n",
      "Epoch 3/5, Batch 2100/4167, Loss: 4.4684\n",
      "Epoch 3/5, Batch 2200/4167, Loss: 4.4847\n",
      "Epoch 3/5, Batch 2300/4167, Loss: 4.5038\n",
      "Epoch 3/5, Batch 2400/4167, Loss: 4.5213\n",
      "Epoch 3/5, Batch 2500/4167, Loss: 4.5357\n",
      "Epoch 3/5, Batch 2600/4167, Loss: 4.5502\n",
      "Epoch 3/5, Batch 2700/4167, Loss: 4.5650\n",
      "Epoch 3/5, Batch 2800/4167, Loss: 4.5813\n",
      "Epoch 3/5, Batch 2900/4167, Loss: 4.5958\n",
      "Epoch 3/5, Batch 3000/4167, Loss: 4.6107\n",
      "Epoch 3/5, Batch 3100/4167, Loss: 4.6249\n",
      "Epoch 3/5, Batch 3200/4167, Loss: 4.6385\n",
      "Epoch 3/5, Batch 3300/4167, Loss: 4.6510\n",
      "Epoch 3/5, Batch 3400/4167, Loss: 4.6643\n",
      "Epoch 3/5, Batch 3500/4167, Loss: 4.6765\n",
      "Epoch 3/5, Batch 3600/4167, Loss: 4.6896\n",
      "Epoch 3/5, Batch 3700/4167, Loss: 4.7023\n",
      "Epoch 3/5, Batch 3800/4167, Loss: 4.7140\n",
      "Epoch 3/5, Batch 3900/4167, Loss: 4.7261\n",
      "Epoch 3/5, Batch 4000/4167, Loss: 4.7389\n",
      "Epoch 3/5, Batch 4100/4167, Loss: 4.7517\n",
      "Epoch 3 completed, Total Loss: 19842.7574\n",
      "Epoch 4/5 started...\n",
      "Epoch 4/5, Batch 0/4167, Loss: 0.3471\n",
      "Epoch 4/5, Batch 100/4167, Loss: 3.0743\n",
      "Epoch 4/5, Batch 200/4167, Loss: 3.4035\n",
      "Epoch 4/5, Batch 300/4167, Loss: 3.5060\n",
      "Epoch 4/5, Batch 400/4167, Loss: 3.6418\n",
      "Epoch 4/5, Batch 500/4167, Loss: 3.7581\n",
      "Epoch 4/5, Batch 600/4167, Loss: 3.7944\n",
      "Epoch 4/5, Batch 700/4167, Loss: 3.8709\n",
      "Epoch 4/5, Batch 800/4167, Loss: 3.8985\n",
      "Epoch 4/5, Batch 900/4167, Loss: 3.9627\n",
      "Epoch 4/5, Batch 1000/4167, Loss: 3.9821\n",
      "Epoch 4/5, Batch 1100/4167, Loss: 4.0268\n",
      "Epoch 4/5, Batch 1200/4167, Loss: 4.0508\n",
      "Epoch 4/5, Batch 1300/4167, Loss: 4.0930\n",
      "Epoch 4/5, Batch 1400/4167, Loss: 4.1125\n",
      "Epoch 4/5, Batch 1500/4167, Loss: 4.1450\n",
      "Epoch 4/5, Batch 1600/4167, Loss: 4.1632\n",
      "Epoch 4/5, Batch 1700/4167, Loss: 4.1906\n",
      "Epoch 4/5, Batch 1800/4167, Loss: 4.2136\n",
      "Epoch 4/5, Batch 1900/4167, Loss: 4.2337\n",
      "Epoch 4/5, Batch 2000/4167, Loss: 4.2534\n",
      "Epoch 4/5, Batch 2100/4167, Loss: 4.2725\n",
      "Epoch 4/5, Batch 2200/4167, Loss: 4.2919\n",
      "Epoch 4/5, Batch 2300/4167, Loss: 4.3120\n",
      "Epoch 4/5, Batch 2400/4167, Loss: 4.3298\n",
      "Epoch 4/5, Batch 2500/4167, Loss: 4.3463\n",
      "Epoch 4/5, Batch 2600/4167, Loss: 4.3613\n",
      "Epoch 4/5, Batch 2700/4167, Loss: 4.3797\n",
      "Epoch 4/5, Batch 2800/4167, Loss: 4.3969\n",
      "Epoch 4/5, Batch 2900/4167, Loss: 4.4134\n",
      "Epoch 4/5, Batch 3000/4167, Loss: 4.4297\n",
      "Epoch 4/5, Batch 3100/4167, Loss: 4.4451\n",
      "Epoch 4/5, Batch 3200/4167, Loss: 4.4605\n",
      "Epoch 4/5, Batch 3300/4167, Loss: 4.4752\n",
      "Epoch 4/5, Batch 3400/4167, Loss: 4.4901\n",
      "Epoch 4/5, Batch 3500/4167, Loss: 4.5040\n",
      "Epoch 4/5, Batch 3600/4167, Loss: 4.5185\n",
      "Epoch 4/5, Batch 3700/4167, Loss: 4.5327\n",
      "Epoch 4/5, Batch 3800/4167, Loss: 4.5473\n",
      "Epoch 4/5, Batch 3900/4167, Loss: 4.5611\n",
      "Epoch 4/5, Batch 4000/4167, Loss: 4.5753\n",
      "Epoch 4/5, Batch 4100/4167, Loss: 4.5896\n",
      "Epoch 4 completed, Total Loss: 19171.8705\n",
      "Epoch 5/5 started...\n",
      "Epoch 5/5, Batch 0/4167, Loss: 0.1119\n",
      "Epoch 5/5, Batch 100/4167, Loss: 2.9532\n",
      "Epoch 5/5, Batch 200/4167, Loss: 3.2464\n",
      "Epoch 5/5, Batch 300/4167, Loss: 3.3539\n",
      "Epoch 5/5, Batch 400/4167, Loss: 3.4978\n",
      "Epoch 5/5, Batch 500/4167, Loss: 3.6113\n",
      "Epoch 5/5, Batch 600/4167, Loss: 3.6496\n",
      "Epoch 5/5, Batch 700/4167, Loss: 3.7333\n",
      "Epoch 5/5, Batch 800/4167, Loss: 3.7581\n",
      "Epoch 5/5, Batch 900/4167, Loss: 3.8203\n",
      "Epoch 5/5, Batch 1000/4167, Loss: 3.8401\n",
      "Epoch 5/5, Batch 1100/4167, Loss: 3.8865\n",
      "Epoch 5/5, Batch 1200/4167, Loss: 3.9070\n",
      "Epoch 5/5, Batch 1300/4167, Loss: 3.9486\n",
      "Epoch 5/5, Batch 1400/4167, Loss: 3.9653\n",
      "Epoch 5/5, Batch 1500/4167, Loss: 3.9988\n",
      "Epoch 5/5, Batch 1600/4167, Loss: 4.0179\n",
      "Epoch 5/5, Batch 1700/4167, Loss: 4.0471\n",
      "Epoch 5/5, Batch 1800/4167, Loss: 4.0709\n",
      "Epoch 5/5, Batch 1900/4167, Loss: 4.0933\n",
      "Epoch 5/5, Batch 2000/4167, Loss: 4.1180\n",
      "Epoch 5/5, Batch 2100/4167, Loss: 4.1386\n",
      "Epoch 5/5, Batch 2200/4167, Loss: 4.1576\n",
      "Epoch 5/5, Batch 2300/4167, Loss: 4.1784\n",
      "Epoch 5/5, Batch 2400/4167, Loss: 4.1988\n",
      "Epoch 5/5, Batch 2500/4167, Loss: 4.2167\n",
      "Epoch 5/5, Batch 2600/4167, Loss: 4.2340\n",
      "Epoch 5/5, Batch 2700/4167, Loss: 4.2519\n",
      "Epoch 5/5, Batch 2800/4167, Loss: 4.2695\n",
      "Epoch 5/5, Batch 2900/4167, Loss: 4.2868\n",
      "Epoch 5/5, Batch 3000/4167, Loss: 4.3043\n",
      "Epoch 5/5, Batch 3100/4167, Loss: 4.3210\n",
      "Epoch 5/5, Batch 3200/4167, Loss: 4.3376\n",
      "Epoch 5/5, Batch 3300/4167, Loss: 4.3533\n",
      "Epoch 5/5, Batch 3400/4167, Loss: 4.3700\n",
      "Epoch 5/5, Batch 3500/4167, Loss: 4.3846\n",
      "Epoch 5/5, Batch 3600/4167, Loss: 4.4004\n",
      "Epoch 5/5, Batch 3700/4167, Loss: 4.4156\n",
      "Epoch 5/5, Batch 3800/4167, Loss: 4.4301\n",
      "Epoch 5/5, Batch 3900/4167, Loss: 4.4443\n",
      "Epoch 5/5, Batch 4000/4167, Loss: 4.4594\n",
      "Epoch 5/5, Batch 4100/4167, Loss: 4.4753\n",
      "Epoch 5 completed, Total Loss: 18700.1253\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_again(N_EPOCHS):\n",
    "  device=DEVICE\n",
    "  input_size_encoder = len(vocablist_en)\n",
    "  encoder_embedding_size = 300\n",
    "  hidden_size = 1024\n",
    "  num_layers = 2\n",
    "  encoder_dropout = 0.5\n",
    "\n",
    "  encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
    "                            hidden_size, num_layers, encoder_dropout).to(device)\n",
    "\n",
    "  input_size_decoder = len(vocablist_vi)\n",
    "  decoder_embedding_size = 300\n",
    "  hidden_size = 1024\n",
    "  num_layers = 2\n",
    "  decoder_dropout = 0.5\n",
    "  output_size = len(vocablist_vi)\n",
    "\n",
    "  decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
    "                            hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
    "  learning_rate = 0.001\n",
    "  step = 0\n",
    "  \n",
    "  model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  pad_idx = vocabidx_vi[\"<pad>\"]\n",
    "  criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "  model.load_state_dict(torch.load(\"../input/lstm40/LSTMSeq2Seq40\"))\n",
    "  print(model)\n",
    "  model.train()\n",
    "  for epoch in range(N_EPOCHS):\n",
    "    batchloss = 0\n",
    "    for en, vi in train_data:\n",
    "      \n",
    "      en = torch.tensor(en, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
    "      vi = torch.tensor(vi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
    "      y=model(en,vi)\n",
    "      output = y[1:].reshape(-1, y.shape[2])\n",
    "      target = vi[1:].reshape(-1)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = criterion(output, target)\n",
    "      loss.backward()\n",
    "      \n",
    "      optimizer.step()\n",
    "      batchloss += loss.item()\n",
    "\n",
    "    print(\"epoch\", epoch, \": loss\", batchloss)     \n",
    "train_again(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T05:50:49.639192Z",
     "iopub.status.busy": "2024-12-14T05:50:49.638841Z",
     "iopub.status.idle": "2024-12-14T05:50:50.512978Z",
     "shell.execute_reply": "2024-12-14T05:50:50.512275Z",
     "shell.execute_reply.started": "2024-12-14T05:50:49.639152Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"LSTMSeq2Seq40\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T05:50:52.903825Z",
     "iopub.status.busy": "2024-12-14T05:50:52.903455Z",
     "iopub.status.idle": "2024-12-14T05:50:52.911574Z",
     "shell.execute_reply": "2024-12-14T05:50:52.910744Z",
     "shell.execute_reply.started": "2024-12-14T05:50:52.903793Z"
    },
    "id": "oCwE4aulaRXt"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, src, en, vi, device, max_length=50):\n",
    "  hidden, cell =model.Encoder_LSTM(src)\n",
    "\n",
    "  y = [vocabidx_vi['<cls>']]\n",
    "  pred=[]\n",
    "  for _ in range(max_length):\n",
    "      previous_word = torch.tensor([y[-1]]).to(device)\n",
    "      with torch.no_grad():\n",
    "        output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
    "        best_guess = output.argmax(1).item()\n",
    "        y.append(best_guess)\n",
    "        if output.argmax(1).item() == vocabidx_vi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "  translated_sentence =[]\n",
    "  for idx in y:\n",
    "    \n",
    "    if vocablist_en[idx] and vocablist_en[idx][0]!='<unk>':\n",
    "      translated_sentence.append(vocablist_vi[idx][0])\n",
    "      if len(translated_sentence)>1:\n",
    "        if vocablist_vi[idx][0] ==translated_sentence[-2]:\n",
    "          translated_sentence.pop(-1)\n",
    "    elif len(translated_sentence)>=4:\n",
    "      if vocablist_vi[idx][0] ==translated_sentence[-3] and translated_sentence[-2] ==translated_sentence[-4]:\n",
    "        print( translated_sentence[-1],translated_sentence[-3], translated_sentence[-2],translated_sentence[-4])\n",
    "        translated_sentence.pop(-1)\n",
    "        translated_sentence.pop(-2)\n",
    "\n",
    "  \n",
    "  return translated_sentence[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T05:50:57.816629Z",
     "iopub.status.busy": "2024-12-14T05:50:57.816294Z",
     "iopub.status.idle": "2024-12-14T05:51:27.611791Z",
     "shell.execute_reply": "2024-12-14T05:51:27.610879Z",
     "shell.execute_reply.started": "2024-12-14T05:50:57.816598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  1268\n",
      "bleu:  0.031116661699279704\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "  model.eval()\n",
    "  ref = []\n",
    "  pred = []\n",
    "  for enprep, en, vi in test_data:\n",
    "    input = torch.tensor([enprep], dtype=torch.int64).transpose(0,1).to(device)\n",
    "    p = evaluate(model, input,en,vi, device)\n",
    "    # p = model.evaluate(model, input, maxlen,start_symbol, vocablist_vi, vocabidx_vi)\n",
    "    ref.append([vi])\n",
    "    pred.append(p)\n",
    "  bleu = torchtext.data.metrics.bleu_score(pred, ref)\n",
    "  print(\"total: \", len(test_data))\n",
    "  print(\"bleu: \", bleu)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T05:53:51.554612Z",
     "iopub.status.busy": "2024-12-14T05:53:51.554251Z",
     "iopub.status.idle": "2024-12-14T05:53:52.442172Z",
     "shell.execute_reply": "2024-12-14T05:53:52.441481Z",
     "shell.execute_reply.started": "2024-12-14T05:53:51.554579Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"LSTMSeq2Seq50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bleu_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_and_evaluate\u001b[39m(model, test_data, vocablist_en, vocablist_vi, vocabidx_vi, device, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Chuyển model sang chế độ đánh giá\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def translate_and_evaluate(model, test_data, vocablist_en, vocablist_vi, vocabidx_vi, device, num_samples=5):\n",
    "    model.eval()  # Chuyển model sang chế độ đánh giá\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    sample_data = test_data[:num_samples]\n",
    "    \n",
    "    for enprep, en, vi in sample_data:\n",
    "        input_tensor = torch.tensor([enprep], dtype=torch.int64).transpose(0, 1).to(device) \n",
    "        translated_sentence = evaluate(model, input_tensor, en, vi, device) \n",
    "        predictions.append(translated_sentence) \n",
    "        references.append([vi]) \n",
    "    \n",
    "    # BLEU\n",
    "    bleu = bleu_score(predictions, references) \n",
    "    print(f\"BLEU score for {num_samples} samples: {bleu:.4f}\")\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        print(f\"Input sentence ({i+1}): {' '.join(vocablist_en[token][0] for token in test_data[i][0])}\")\n",
    "        print(f\"Translated sentence ({i+1}): {' '.join(predictions[i])}\")\n",
    "        print(f\"Reference sentence ({i+1}): {' '.join(references[i][0])}\")\n",
    "        print(\"\")\n",
    "\n",
    "translate_and_evaluate(model, test_data, vocablist_en, vocablist_vi, vocabidx_vi, device, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1452269,
     "sourceId": 2401553,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1452271,
     "sourceId": 2401556,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6293358,
     "sourceId": 10186918,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30097,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
